# Moderation-Application


This project involves building a deep learning model for social media comment moderation. The goal is to classify user comments as either toxic or non-toxic. If a comment is toxic, the model provides detailed reasons, such as sexually explicit content, harm to minority or religious communities, offensive language towards the LGBTQ community, derogatory remarks on physical appearance, and bashing of castes and creeds. Moreover, with the usage of LIME (Local Interpretable Model-agnostic Explanations), the model can predict which words have the highest likelihood and percentage weight of making a comment toxic.
